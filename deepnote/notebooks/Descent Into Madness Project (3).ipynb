{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqKfGO-3tU1W",
        "cell_id": "61d7e1f8bf4844be8163a9a6ca32ec3a",
        "deepnote_cell_type": "markdown"
      },
      "source": "# **Predict2Win: Predicting Next Events in a Football Match**",
      "block_group": "f9ee26c5889a4a3284bca676b15c448b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqStt3gktU1Z",
        "cell_id": "84f27099de9a42f881f52eae059c905f",
        "deepnote_cell_type": "markdown"
      },
      "source": "**Table of Contents**\n0. Background\n1. Data import\n2. Data preparation\n3. Methods  \n    3.1 Single Layer Baseline\n    3.2 LSTM\n    3.3 Transformer\n4. Results\n5. Discussion\n6. Division of labour\n7. References",
      "block_group": "77005d4aaa8c4f03bfd5339bd1e14d50"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbIvxIPytU1Z",
        "cell_id": "2082174fff934dd1889d26deb5c855f0",
        "deepnote_cell_type": "markdown"
      },
      "source": "## Disclaimer  \nParts of this code were adapted from Seq2Event (Simpson et al., 2022), a project following the same goal as us. No code was copy pasted, only the logic was adapted. Furthermore, parts of this code were written with the help of AI. No code was copy pasted directly from AI.",
      "block_group": "3518612e19094efb8522a67e4d81a0f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnmn81fZtU1Z",
        "cell_id": "5e64409a70f9494da7bbd51d47f3b79f",
        "deepnote_cell_type": "markdown"
      },
      "source": "### Background\nThe goal of this project was to build a machine learning model that could predict the next event in a match (e.g. shot or pass), given some preceding events.\nTrying to predict football matches is common and in fact a lot of information about potential game outcomes is presented before the game (e.g. bet projections) and during the game (e.g. expected goals). And still, if predictions like these were accurate, companies offering sport bets would go bankrupt. Clearly, there is yet to be an algorithm that outperforms the state of the art.\nFootball matches are inherently noisy. Players make mistakes, referees make questionable calls and sometimes the outcome of a match boils down to dumb luck. Still, previous literature (Simpson et al., 2022) has shown that machine learning models are able to find structure in the data and make accurate predictions about what will happen next in the match. In this project, we are trying to apply the logic of a next-word prediction model like GPT2 to football data and build a custom transformer model to predict the next event, given a sequence of previous events. Following this, the performance will be compared to simpler architectures, namely an LSTM and a two-layer fully connected network. The data being used in this project stems from WYSCOUT (Pappalardo et al., 2019), a company aiming to build up large sports datasets. Direct python integration stems from [Kloppy](https://kloppy.pysport.org/) [(PySport)](https://pysport.org/), providing a subset of the complete WYSCOUT dataset. Data includes about 2000 matches from the Top 5 european leagues as well as data from the FIFA World Cup 2018 and the UEFA Euro Cup 2020.",
      "block_group": "b4252f8d9dd4451ea4d2a76a63f01193"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZI87h2BtU1a",
        "cell_id": "b247bf066a9b473c8b0bb04ee76333a6",
        "deepnote_cell_type": "markdown"
      },
      "source": "### Installing the requirements:\nThis project is uploaded to a [GitHub repository](https://github.com/jvli4n/DLiP2025_football_analysis). To run this notebook in the way it was intended, the notebook will install the dependencies from a requirements.txt within this repository.",
      "block_group": "8dcbffaacb8c4e8d9d9438045783daec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v15FaxFrtU1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8830211-b67e-45f7-d9eb-0d240ed45089",
        "source_hash": "2c8b407e",
        "execution_start": 1766272526853,
        "execution_millis": 8193,
        "execution_context_id": "8deba546-e814-498f-8228-d50a7b669d49",
        "cell_id": "b1de6359e4fd4752a3ff6635d29645a7",
        "deepnote_cell_type": "code"
      },
      "source": "!pip install -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt",
      "block_group": "28a35e33239440f0bdfd17aaebe579b2",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: numpy in /root/venv/lib/python3.11/site-packages (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: pandas in /root/venv/lib/python3.11/site-packages (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 2)) (2.1.4)\nRequirement already satisfied: matplotlib in /root/venv/lib/python3.11/site-packages (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: seaborn in /root/venv/lib/python3.11/site-packages (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 4)) (0.13.2)\nCollecting pathlib (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 5))\n  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: tqdm in /root/venv/lib/python3.11/site-packages (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 6)) (4.67.1)\nRequirement already satisfied: IPython in /root/venv/lib/python3.11/site-packages (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (9.7.0)\nCollecting kloppy (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9))\n  Downloading kloppy-3.18.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: scikit-learn in /root/venv/lib/python3.11/site-packages (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 10)) (1.3.2)\nRequirement already satisfied: torch in /root/venv/lib/python3.11/site-packages (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (2.1.2)\nCollecting torchmetrics (from -r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 12))\n  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /root/venv/lib/python3.11/site-packages (from pandas->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 2)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /root/venv/lib/python3.11/site-packages (from pandas->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 2)) (2025.2)\nRequirement already satisfied: tzdata>=2022.1 in /root/venv/lib/python3.11/site-packages (from pandas->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 2)) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /root/venv/lib/python3.11/site-packages (from matplotlib->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: cycler>=0.10 in /root/venv/lib/python3.11/site-packages (from matplotlib->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 3)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /root/venv/lib/python3.11/site-packages (from matplotlib->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 3)) (4.60.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /root/venv/lib/python3.11/site-packages (from matplotlib->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 3)) (1.4.9)\nRequirement already satisfied: packaging>=20.0 in /root/venv/lib/python3.11/site-packages (from matplotlib->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 3)) (25.0)\nRequirement already satisfied: pillow>=8 in /root/venv/lib/python3.11/site-packages (from matplotlib->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 3)) (12.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /root/venv/lib/python3.11/site-packages (from matplotlib->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 3)) (3.2.5)\nRequirement already satisfied: decorator>=4.3.2 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (5.2.1)\nRequirement already satisfied: ipython-pygments-lexers>=1.0.0 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (1.1.1)\nRequirement already satisfied: jedi>=0.18.1 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (0.19.2)\nRequirement already satisfied: matplotlib-inline>=0.1.5 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (0.2.1)\nRequirement already satisfied: pexpect>4.3 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (4.9.0)\nRequirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (3.0.52)\nRequirement already satisfied: pygments>=2.11.0 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (2.19.2)\nRequirement already satisfied: stack_data>=0.6.0 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (0.6.3)\nRequirement already satisfied: traitlets>=5.13.0 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (5.14.3)\nRequirement already satisfied: typing_extensions>=4.6 in /root/venv/lib/python3.11/site-packages (from IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (4.15.0)\nRequirement already satisfied: lxml>=4.4.0 in /root/venv/lib/python3.11/site-packages (from kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9)) (6.0.2)\nRequirement already satisfied: sortedcontainers>=2 in /root/venv/lib/python3.11/site-packages (from kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9)) (2.4.0)\nRequirement already satisfied: fsspec>=2024.12.0 in /root/venv/lib/python3.11/site-packages (from fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9)) (2025.10.0)\nRequirement already satisfied: scipy>=1.5.0 in /root/venv/lib/python3.11/site-packages (from scikit-learn->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 10)) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /root/venv/lib/python3.11/site-packages (from scikit-learn->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 10)) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /root/venv/lib/python3.11/site-packages (from scikit-learn->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 10)) (3.6.0)\nRequirement already satisfied: filelock in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (3.20.0)\nRequirement already satisfied: sympy in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (1.12)\nRequirement already satisfied: networkx in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (3.6)\nRequirement already satisfied: jinja2 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (2.18.1)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (12.1.105)\nRequirement already satisfied: triton==2.1.0 in /root/venv/lib/python3.11/site-packages (from torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (2.1.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /root/venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (12.9.86)\nCollecting lightning-utilities>=0.8.0 (from torchmetrics->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 12))\n  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\nCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9))\n  Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /root/venv/lib/python3.11/site-packages (from jedi>=0.18.1->IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (0.8.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 12)) (65.5.1)\nRequirement already satisfied: ptyprocess>=0.5 in /root/venv/lib/python3.11/site-packages (from pexpect>4.3->IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (0.7.0)\nRequirement already satisfied: wcwidth in /root/venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (0.2.14)\nRequirement already satisfied: six>=1.5 in /root/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 2)) (1.17.0)\nRequirement already satisfied: executing>=1.2.0 in /root/venv/lib/python3.11/site-packages (from stack_data>=0.6.0->IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (2.2.1)\nRequirement already satisfied: asttokens>=2.1.0 in /root/venv/lib/python3.11/site-packages (from stack_data>=0.6.0->IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (3.0.1)\nRequirement already satisfied: pure-eval in /root/venv/lib/python3.11/site-packages (from stack_data>=0.6.0->IPython->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 7)) (0.2.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /root/venv/lib/python3.11/site-packages (from jinja2->torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (3.0.3)\nRequirement already satisfied: mpmath>=0.19 in /root/venv/lib/python3.11/site-packages (from sympy->torch->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 11)) (1.3.0)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9))\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9))\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: attrs>=17.3.0 in /root/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9)) (25.4.0)\nCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9))\n  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9))\n  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9))\n  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9))\n  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: idna>=2.0 in /root/venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2024.12.0->kloppy->-r https://raw.githubusercontent.com/jvli4n/DLiP2025_football_analysis/refs/heads/main/requirements.txt (line 9)) (3.11)\nDownloading pathlib-1.0.1-py3-none-any.whl (14 kB)\nDownloading kloppy-3.18.0-py3-none-any.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.7/294.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\nDownloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.7/246.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.8/365.8 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pathlib, propcache, multidict, lightning-utilities, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torchmetrics, kloppy\nSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 frozenlist-1.8.0 kloppy-3.18.0 lightning-utilities-0.15.2 multidict-6.7.0 pathlib-1.0.1 propcache-0.4.1 torchmetrics-1.8.2 yarl-1.22.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE4dAeEeE7uw",
        "cell_id": "908115620a7b46288c5de3b856efb897",
        "deepnote_cell_type": "markdown"
      },
      "source": "Next, import the relevant distributions:",
      "block_group": "2bbc273b3997477784d381ee26c50cd0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OtBZoko5nse",
        "source_hash": "59216f7a",
        "execution_start": 1766272539247,
        "execution_millis": 7894,
        "execution_context_id": "8deba546-e814-498f-8228-d50a7b669d49",
        "cell_id": "b6e614a9e05048bc850603a6c2e7bb83",
        "deepnote_cell_type": "code"
      },
      "source": "# Standard Python Library\nimport os\nimport time\nimport copy\nimport json\nimport pickle\nimport requests\nimport warnings\n\n# Other Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom zipfile import ZipFile\n\nfrom tqdm.auto import tqdm\nfrom IPython.display import clear_output\nfrom pandas.api.types import is_categorical_dtype\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom kloppy import wyscout\nfrom kloppy.domain import Provider, Dimension, NormalizedPitchDimensions, Orientation\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.cuda.amp import GradScaler\n\nfrom torchmetrics.classification import ConfusionMatrix",
      "block_group": "790d2a69b2a14788aa63e7fbcc7af22e",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stderr",
          "text": "/root/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYag6SewFCcF",
        "cell_id": "afac5d27ac38417eb94dae59a9daec89",
        "deepnote_cell_type": "markdown"
      },
      "source": "## PART 1: DATA IMPORT",
      "block_group": "6018ed2b09c74a239d6eddfb603cd012"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxor6UUItU1c",
        "cell_id": "23ca3abd195c483084a89facc1657c70",
        "deepnote_cell_type": "markdown"
      },
      "source": "**Please note that running the data import and transformation will take approximately 13 minutes. A pickle file is provided below so the data import can be skipped. However, you still need to define the `download_zip` function below.**",
      "block_group": "1605982976f543e78b21bf5bd5f76a9a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_qpcaLk5nse",
        "source_hash": "ea3b530d",
        "execution_start": 1766272550504,
        "execution_millis": 1,
        "execution_context_id": "8deba546-e814-498f-8228-d50a7b669d49",
        "cell_id": "a45a14b2e8b9446d9c26ff4fd94330ef",
        "deepnote_cell_type": "code"
      },
      "source": "def download_zip(url, save_path):\n  \"\"\"\n  Helper function to download and unzip files from a url.\n  \"\"\"\n\n  # save location\n  save_dir = Path(save_path)\n  save_dir.mkdir(parents=True, exist_ok=True)\n\n  # create the zip folder path\n  zip_name = url.split(\"/\")[-1]\n  zip_stem = zip_name.rsplit(\".\", 1)[0]\n  zip_path = save_dir / zip_name\n\n  # create folder to unzip\n  unzip_dir = save_dir / zip_stem\n  os.makedirs(unzip_dir, exist_ok=True)\n\n  print(save_dir, zip_name, zip_stem, zip_path, unzip_dir)\n\n  response = requests.get(url) # send GET request\n  response.raise_for_status()  # error if download failed\n\n  # download zip file\n  with open(zip_path, 'wb') as f:\n   f.write(response.content)\n\n  # unzip file\n  with ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(unzip_dir)\n\n  print(f\"Downloaded: {url.split('/')[-1]}\")",
      "block_group": "018a03f9af2144dcb165ff5133d0938d",
      "execution_count": 7,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgwwXoKhtU1c",
        "cell_id": "883728c2a8944e04beaac783bcc19951",
        "deepnote_cell_type": "markdown"
      },
      "source": "Pappalardo et al. (2019) made (part of) their data publicly available on figshare. Below is a code cell to download the data needed for this notebook. **Please specify the path where you would like to save the data before running the cell in case it should not be saved in the current working directory!**",
      "block_group": "ba996c95edb2412a884b9dd43df13fb9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8cxl7hz5nse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd0e702-2d36-4c05-e9da-3ed934c872f4",
        "source_hash": "a22674d",
        "execution_start": 1765636175040,
        "execution_millis": 159817,
        "execution_context_id": "2abe5b25-404b-461b-b7e9-88facedd67ed",
        "cell_id": "6ac48e115329462aadbda2cbd5d5ab53",
        "deepnote_cell_type": "code"
      },
      "source": "project_path = os.getcwd() # specify path where data should be saved\n\n# match ids for referencing the matches after extracting with kloppy\nmatches_url = \"https://figshare.com/ndownloader/files/14464622/matches.zip\"\ndownload_zip(matches_url, project_path)",
      "block_group": "8190ac8bf63f4f02a7a039731a083710",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/content matches.zip matches /content/matches.zip /content/matches\nDownloaded: matches.zip\n"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLGkK76rtU1c",
        "cell_id": "924d1ad72d084aff96bdba20e071110a",
        "deepnote_cell_type": "markdown"
      },
      "source": "This function is used as a helper function for the next block because distributing it across mutliple CPU cores we need the actual function to load a match separately. After the matches are loaded, they are also transformed with some kloppy functions. First the pitch dimensions are normalized because some providers give different pitch dimensions. Next a standard coordinate system is used for the same reason. Orientations are made static (no halftime switch), this is because the model might get fed a data window from overlapping periods and like this we don't have to specifiy periods and save some memmory.",
      "block_group": "0fc4901dfd4c447c9e514bde52c43dff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dkw6VFltU1c",
        "cell_id": "cc7bf622d0b9467b9e5c8e3d5b76e170",
        "deepnote_cell_type": "code"
      },
      "source": "def load_match(match_id):\n    \"\"\"Helper function for loading a single match with kloppy.\"\"\"\n    try:\n        dataset = wyscout.load_open_data(match_id=match_id, coordinates=\"wyscout\")\n\n        # Normalize data\n        dataset = dataset.transform(to_pitch_dimensions=NormalizedPitchDimensions(pitch_length=105,\n                                                        pitch_width=68,\n                                                        x_dim = Dimension(0, 1),\n                                                        y_dim = Dimension(0, 1)),\n                                    to_coordinate_system = Provider.WYSCOUT,\n                                    to_orientation = Orientation.STATIC_HOME_AWAY)\n        dataset = dataset.to_df()\n        return match_id, dataset\n\n    except Exception as e:\n        print(f\"Failed match {match_id}: {e}\")\n        return match_id, None\n",
      "block_group": "0ef0bd264a1d465d8bc1a5f0d90f6658",
      "execution_count": 5,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATGkC8UET_3n",
        "cell_id": "d860b98924704b31bf768fc42ad0dacc",
        "deepnote_cell_type": "code"
      },
      "source": "def load_kloppy(matches_path, save_path):\n  \"\"\"\n  Helper function to load and normalize match data from wyscout using kloppy.\n  Saves everything into a dictionary with match ids as keys and dataframes for each match as values.\n  The dictionary is saved as a pickle file at save_path.\n  \"\"\"\n  # The first part get's the matchIDs so we can use them later to load single matches\n  matches_dir = Path(matches_path)\n  match_ids = [] # buffer for match ids\n\n  # collect json files containing matchid\n  for comp in matches_dir.glob(\"*.json\"): # find and loop through .json files\n    with comp.open(\"r\") as f:\n      data = json.load(f) # load json\n\n      for mtch in data:\n        match_ids.append(mtch[\"wyId\"]) #Append unique matchID\n\n  data_dict = {}\n  # Parallelize data import and transform\n  with ThreadPoolExecutor(max_workers=None) as ex: # Default = all workers\n    for match_id, df in tqdm(ex.map(load_match, match_ids), #tqdm creates the loading buffer\n                             total=len(match_ids),\n                             desc=\"Loading matches\"):\n      if df is None:\n        continue\n      data_dict[match_id] = df # Data fictionary with matchID as key and df as value\n\n  print(f\"Total matches loaded: {len(data_dict)}\")\n\n  # save dictionary as .pkl\n  with open(save_path, \"wb\") as f:\n    pickle.dump(data_dict, f)\n\n  print(f\"Dictionary saved at {save_path}\")",
      "block_group": "f4c6b5de77394f278691881080b793cc",
      "execution_count": 6,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwMVEM5ypgd6",
        "colab": {
          "height": 468,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "c5b54a74425041eea56863ea531f3b61",
            "82c419ab591c4ecf90d77771b0b3dfab",
            "4053dde9f1d14e2a963965ac0b0d34f0",
            "3363c35754f64b1182147c4171bfba4a",
            "889fd4be22434141b660744dc5b96135",
            "cf12b0d95d164847bd4f92c7b1eafae8",
            "dda372ec038442e98dc8014e9385def8",
            "01f3949996724274a3d9e4c82f3c68d0",
            "83e547e31d984c2c96ddb3b8297d5ee8",
            "81357866b0d24847a1d209fc706b8917",
            "14c447ee3b354ed088b302713f339a1e"
          ]
        },
        "outputId": "6bd8b439-4688-4b8d-a8c9-0f9e9710427d",
        "cell_id": "b8010f71caa14449aa51c5284e38e6e1",
        "deepnote_cell_type": "code"
      },
      "source": "# load the data using the function\nmatches_path = os.path.join(project_path, \"matches\")\ndict_path = os.path.join(project_path, \"wyscout_data.pkl\")\nload_kloppy(matches_path, dict_path)",
      "block_group": "50effb45ab99405ebd16a5397269bc51",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading matches:   0%|          | 0/1941 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5b54a74425041eea56863ea531f3b61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Failed match 2576332: 'NoneType' object is not subscriptable\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3024500516.py\u001b[0m in \u001b[0;36mload_kloppy\u001b[0;34m(matches_path, save_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Default = all workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     for match_id, df in tqdm(ex.map(load_match, match_ids), #tqdm creates the loading buffer\n\u001b[0m\u001b[1;32m     23\u001b[0m                              \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2022424533.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmatches_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matches\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdict_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wyscout_data.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mload_kloppy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3024500516.py\u001b[0m in \u001b[0;36mload_kloppy\u001b[0;34m(matches_path, save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# Parallelize data import and transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Default = all workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     for match_id, df in tqdm(ex.map(load_match, match_ids), #tqdm creates the loading buffer\n\u001b[1;32m     23\u001b[0m                              \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLvoqkCiQdIa",
        "cell_id": "b6bb92bd1af047378baecdb79541b6d8",
        "deepnote_cell_type": "markdown"
      },
      "source": "**NOTE: IF WYSCOUT_DATA DICTIONARY IS SAVED ON DISK START FROM HERE TO SAVE TIME**\n\nThe code cell below downloads a dictionary from our GitHub repo that contains the matchids as keys and a dataframe with the corresponding events as values. **Make sure to specify a path where you would like to save the .zip file in case it should not be the current working directory.**",
      "block_group": "b4a3795625e34d4596e81ed0baa52fa3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clOkEYHmtU1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4b4854-2041-4ad5-8a64-d0efe2855dff",
        "source_hash": "b5689dcf",
        "execution_start": 1766272562603,
        "execution_millis": 10126,
        "execution_context_id": "8deba546-e814-498f-8228-d50a7b669d49",
        "cell_id": "dfbb57586e924c1f87a014396dfbf869",
        "deepnote_cell_type": "code"
      },
      "source": "project_path = os.getcwd() # specify path where data should be saved\n\n# data is provided in GitHub repo\ngithub_link = 'https://github.com/jvli4n/DLiP2025_football_analysis/raw/refs/heads/main/wyscout_data.zip'\n\n# download zip from GitHub\ndownload_zip(github_link, project_path)\n\n# extract the data_dict from the .zip file\ndict_path = os.path.join(project_path, \"wyscout_data.zip\")\nwith ZipFile(dict_path) as zf:\n    with zf.open(\"data/wyscout_data.pkl\") as file:\n        data_dict = pd.read_pickle(file)\n\n# example match (id: 2058017)\nprint(data_dict[2058017].head(5))",
      "block_group": "5ad75891dc6442e98d1339b6f4cc2f5e",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "text": "/datasets/_deepnote_work wyscout_data.zip wyscout_data /datasets/_deepnote_work/wyscout_data.zip /datasets/_deepnote_work/wyscout_data\nDownloaded: wyscout_data.zip\n    event_id event_type  period_id              timestamp end_timestamp  \\\n0  263883958       PASS          1 0 days 00:00:01.892339          None   \n1  263883959       PASS          1 0 days 00:00:03.889375          None   \n2  263883960       PASS          1 0 days 00:00:06.140946          None   \n3  263883963       PASS          1 0 days 00:00:09.226570          None   \n4  263883964       PASS          1 0 days 00:00:12.658969          None   \n\n  ball_state ball_owning_team team_id player_id  coordinates_x  ...  \\\n0       None             None    9598     14943       0.500000  ...   \n1       None             None    9598     69968       0.607521  ...   \n2       None             None    9598      8287       0.654693  ...   \n3       None             None    9598     69409       0.692430  ...   \n4       None             None    9598    135747       0.886508  ...   \n\n   receiver_player_id  is_counter_attack    pass_type    result success  \\\n0                None              False  SIMPLE_PASS      None    None   \n1                None              False  SIMPLE_PASS  COMPLETE    True   \n2                None              False  SIMPLE_PASS  COMPLETE    True   \n3                None              False  SIMPLE_PASS  COMPLETE    True   \n4                None              False       LAUNCH  COMPLETE    True   \n\n  duel_type set_piece_type body_part_type goalkeeper_type card_type  \n0      None           None           None            None      None  \n1      None           None           None            None      None  \n2      None           None           None            None      None  \n3      None           None           None            None      None  \n4      None           None           None            None      None  \n\n[5 rows x 23 columns]\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKIVw1gUtU1d",
        "cell_id": "f7a372f074684a3a8e31a43fc4a4f6b9",
        "deepnote_cell_type": "markdown"
      },
      "source": "### Model Hyperparameters\n**Please change them here globally**  \n`LEARNING RATE` and `PATIENCE` were used because they seemed to yield the best results during tinkering.",
      "block_group": "a15b96c3f7a047b9933a1c9e8ae6b829"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU5TMCRBtU1d",
        "source_hash": "26ddbd49",
        "execution_start": 1766272579312,
        "execution_millis": 1,
        "execution_context_id": "8deba546-e814-498f-8228-d50a7b669d49",
        "cell_id": "4ce44bac9b9c4b538c24cb549af5c240",
        "deepnote_cell_type": "code"
      },
      "source": "WINDOW_SIZE = 30\nLEARNING_RATE = 1e-4\nPATIENCE = 2 # Wait to epochs before chagning learning rate\nDROPOUT_RATE = 0.1 # 10%\nBATCH_SIZE = 512 # More = speed\nTRANSFORMER_DIMENSION = 128 # Hidden dimension\nLSTM_HIDDEN_DIM = None\nLSTM_N_CELLS = 2",
      "block_group": "ba834f6b4de049b487717a6fac57d3aa",
      "execution_count": 13,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gqGwW2vQy28",
        "cell_id": "959cef55f0e34344971380f0a66564ab",
        "deepnote_cell_type": "markdown"
      },
      "source": "## PART 2: DATA PREPROCESSING",
      "block_group": "5b6b6079ad3c4e7a8052b88ea545b991"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4ja_lGktU1d",
        "cell_id": "95714cd6d6494e3f99d9e6b7a9d204a5",
        "deepnote_cell_type": "markdown"
      },
      "source": "In this cell the data be transformed into a format, usable by the models. First, going from a dictionary with the matchid as keys and the data as values we concatenate all matches below one another, beginning with a `<BOM>` and ending with a `<EOM>` token. Next a column is added containing all the valid indices where a context window can be place across the whole dataframe. A valid window is defined by the window never overlapping across matches. This way the input never includes data from two separate matches. Additionally, timestamps (in seconds) for each event and the length of each event, as well as  the ∆ coordinates are included as separate columns for additional information. All categorical variables are transformed into their category codes, so they can be put into tensors. NAs are either filled with -1 for numerical variables and with a `<missing>` token for categorical variables. The final dataset is of type `pd.DataFrame` and includes only relevant columns needed for further processing or model training.",
      "block_group": "07fdcf583a194d36a49266f118254fa7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ok-SLbq5nsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07726722-4870-4f9b-e6b7-3d3d77fd5452",
        "source_hash": "cd2d0478",
        "execution_start": 1766273255293,
        "execution_millis": 6812,
        "execution_context_id": "8deba546-e814-498f-8228-d50a7b669d49",
        "cell_id": "c3fe5d2659864b6daad24993be662c8b",
        "deepnote_cell_type": "code"
      },
      "source": "# columns of interest\ncols = ['event_type', 'period_id', 'team_id', 'player_id', 'success', 'result', # catecorigal\n            'timestamp', 'coordinates_x', 'coordinates_y'] # numeric\n\n# beginning of match and end of match tokens\nbegin = pd.DataFrame([\"<BOM>\"] + [None] * (len(cols)-1), index=cols).T\nend = pd.DataFrame([\"<EOM>\"] + [None] * (len(cols)-1), index=cols).T\n\nwith warnings.catch_warnings(): # Concatenating with NaN will be depracated in next pandas version\n    warnings.simplefilter(\"ignore\", FutureWarning)\n    for matchid, df in data_dict.items():\n      temp_df = df[cols]\n      temp_df = pd.concat((begin, temp_df, end), ignore_index= True)\n      valid_idx = np.arange(len(temp_df)) + WINDOW_SIZE < len(temp_df)\n      temp_df['valid_idx'] = valid_idx.astype(int)\n      data_dict[matchid] = temp_df\n\n# concatenate all matches and create match_id column\ndata_df = pd.concat([df.assign(match_id = id) for id, df in data_dict.items()], ignore_index = True)\n\n# collapse event_type and result columns to create input_event\ndata_df[\"input_event\"] = data_df.event_type # copy event type column\nfltr = data_df.result.notna() # filter out None results\ndata_df.loc[fltr, \"input_event\"] = data_df.loc[fltr, \"event_type\"] + \"_\" + data_df.loc[fltr, \"result\"] # add result tags to events\n\n# after inspecting the models' performance, we decided to reduce the number of events\n# data_df[\"input_event\"] = data_df.event_type\n# goal =\n\nprint(\"Number of unique events (collapsed):\", len(data_df.input_event.unique()))\n\n# convert timestamps to seconds\ntimes = data_df.timestamp.dt.total_seconds()\ndata_df['timestamp_seconds'] = times\n\n# compute delta time and delta x, y coordinates\ndata_df[\"delta_x\"] = data_df.groupby(\"match_id\").coordinates_x.diff()\ndata_df[\"delta_y\"] = data_df.groupby(\"match_id\").coordinates_y.diff()\ndata_df[\"delta_time\"] = data_df.groupby(\"match_id\").timestamp_seconds.diff()\n\n# convert to categorical dtype\nfactor_cols = [\"team_id\", \"player_id\", \"input_event\"]\ndata_df[factor_cols] = data_df[factor_cols].astype(\"category\")\n\n# drop columns of no interest\ndata_df = data_df.drop(columns = [\"event_type\", \"period_id\", \"success\", \"result\", \"timestamp\"])",
      "block_group": "048cd2db05804fa3b41dcbc84351fd0b",
      "execution_count": 76,
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of unique events (collapsed): 27\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S8Q5ekHq9x1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a8f6ca-7862-4052-9191-d8f729d0c486",
        "source_hash": "8c0feb33",
        "execution_start": 1766273263987,
        "execution_millis": 616,
        "execution_context_id": "8deba546-e814-498f-8228-d50a7b669d49",
        "cell_id": "b6d92beac63f4a708d816c59881f4e5e",
        "deepnote_cell_type": "code"
      },
      "source": "# compute categorical feature dimensions (used later for modelling)\nnum_teams = len(data_df.team_id.unique())\nnum_players = len(data_df.player_id.unique())\nnum_events = len(data_df.input_event.unique())\n\n# replace NA with <missing> token for categorical variables\nfor col in data_df.columns:\n  if isinstance(data_df[col].dtype, pd.CategoricalDtype):\n    data_df[col] = data_df[col].cat.add_categories([\"<missing>\"]).fillna(\"<missing>\")\n\ncodes_map = {}\n\nfor col in data_df.select_dtypes([\"category\"]).columns:\n    data_df[col] = data_df[col].cat.remove_unused_categories()\n    cats = data_df[col].cat.categories # original category\n    codes_map[col] = dict(enumerate(cats)) # mapping\n    data_df[col] = data_df[col].cat.codes # codes\n\nprint(\"\\ncode - label map\")\nprint(pd.Series(codes_map[\"input_event\"]))\n\nprint(\"Number of teams:\", num_teams)\nprint(\"Number of players:\", num_players)\nprint(\"Number of events:\", num_events)\n",
      "block_group": "50bbafc11cd34fcaa02b1fb004f5c948",
      "execution_count": 79,
      "outputs": [
        {
          "name": "stdout",
          "text": "\ncode - label map\n0                    <BOM>\n1                    <EOM>\n2                 BALL_OUT\n3                     CARD\n4                CLEARANCE\n5                     DUEL\n6                DUEL_LOST\n7             DUEL_NEUTRAL\n8                 DUEL_WON\n9           FOUL_COMMITTED\n10         GENERIC:generic\n11              GOALKEEPER\n12       INTERCEPTION_LOST\n13        INTERCEPTION_OUT\n14    INTERCEPTION_SUCCESS\n15              MISCONTROL\n16                    PASS\n17           PASS_COMPLETE\n18         PASS_INCOMPLETE\n19            PASS_OFFSIDE\n20                PASS_OUT\n21                RECOVERY\n22            SHOT_BLOCKED\n23               SHOT_GOAL\n24         SHOT_OFF_TARGET\n25               SHOT_POST\n26              SHOT_SAVED\ndtype: object\nNumber of teams: 143\nNumber of players: 3030\nNumber of events: 27\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Q_1WLM5nsg",
        "source_hash": "fcdd8f26",
        "execution_start": 1765799007480,
        "execution_millis": 164,
        "execution_context_id": "e8669f12-f95b-4655-be6a-90a2376dcdbd",
        "cell_id": "d2d042ef0ee149d19edbc6a98edcdbbd",
        "deepnote_cell_type": "code"
      },
      "source": "# Fill all continuous column NAs with -1\ndata_df = data_df.fillna(-1)\n\n# Create column with labels\nlabel = pd.Series(data_df.input_event[1:].to_numpy())\nlabel[label.index.max() + 1] = data_df.input_event[0:1].item() # shifts labels by 1\nlabel.index = data_df.index # align indices\ndata_df['label'] = label",
      "block_group": "0466a67fc19b4205958182529ed923fd",
      "execution_count": 100,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YmeBdaqEkNp",
        "cell_id": "8edfa5d4b0bd4fd89cbd58d29d1f2ece",
        "deepnote_cell_type": "markdown"
      },
      "source": "**SPLIT INTO TRAIN, VAL AND TEST SETS**",
      "block_group": "03d133af09864e27b463ab3f9ba2ddc5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld9iRRUrtU1e",
        "cell_id": "41d2ee02963947d68f3a8dd85846a1f3",
        "deepnote_cell_type": "markdown"
      },
      "source": "Here we split the data into training (90%) and test (10%). The training set is further split into 75% training and 25% validation set. The test set is not going to be called until the models are finished. The Train / Test split is based on the matches. That means, that instead of dividing based on random events, we decided to keep only entire matches in the respective splits, so the model can learn something about a whole match.",
      "block_group": "45def382ead940a7b61a9e61ae79a8e6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21NlMIio5nsg",
        "source_hash": "a97d4ea1",
        "execution_start": 1765799008186,
        "execution_millis": 306,
        "execution_context_id": "e8669f12-f95b-4655-be6a-90a2376dcdbd",
        "cell_id": "f14746edef384b4db072e3ca4faa7643",
        "deepnote_cell_type": "code"
      },
      "source": "unique_match_ids = data_df.match_id.unique()\n\n# Train-test split\ntrain_ids, test_ids = sklearn.model_selection.train_test_split(unique_match_ids, test_size=0.1) # 10% test data\n\n# Train-validation split\ntrain_ids, val_ids = sklearn.model_selection.train_test_split(train_ids, test_size=0.25) # 25% validation data\n\n# Indexing from data set\nX_train = data_df[data_df.match_id.isin(train_ids)]\nX_test = data_df[data_df.match_id.isin(test_ids)]\nX_val = data_df[data_df.match_id.isin(val_ids)]",
      "block_group": "fa9f3001d30d4ee68b550ca0a2afc096",
      "execution_count": 101,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVg09p69E4_j",
        "cell_id": "4ff645d4ad6b40108134c997ffc52d13",
        "deepnote_cell_type": "markdown"
      },
      "source": "**SPLIT INTO CATEGORICAL AND CONTINUOUS FEATURES**",
      "block_group": "f85f5e7e45994447a064d3e69a43168c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706hlP14tU1e",
        "cell_id": "4bf7fc6197054e958d306099affa1a1a",
        "deepnote_cell_type": "markdown"
      },
      "source": "The features are split here because the categorical features will be fed into an embedding layer, while the continuous features will be fed into linear layer.",
      "block_group": "c2165bf22f00493c884ad9dee2a6dd34"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6iZK6hZ5nsg",
        "source_hash": "4adb3fca",
        "execution_start": 1765799008931,
        "execution_millis": 168,
        "execution_context_id": "e8669f12-f95b-4655-be6a-90a2376dcdbd",
        "cell_id": "970249fe78ae4e5ba1e0bbefde686738",
        "deepnote_cell_type": "code"
      },
      "source": "def split_dataset(df):\n    \"\"\"\n    Function to split up df based on type of features.\n    Returns separate dataframe with categorical features and continuous features\n    in a tuple.\n    \"\"\"\n    # drop columns of no interest\n    df = df.drop(columns = [\"match_id\"])\n\n    # Extract continuous variables into separate df\n    coord_cols = [\n        'coordinates_x',\n        'coordinates_y',\n        'delta_x',\n        'delta_y',\n        'timestamp_seconds',\n        'delta_time']\n    df_cont = df[coord_cols]\n    df_cat = df.drop(columns=coord_cols)\n\n    return df_cat, df_cont\n\nX_train_cat, X_train_cont = split_dataset(X_train)\nX_val_cat,   X_val_cont   = split_dataset(X_val)\nX_test_cat,  X_test_cont  = split_dataset(X_test)",
      "block_group": "84190d54db114a3abaa821923a7746bf",
      "execution_count": 14,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJud3NyUFWuC",
        "cell_id": "37261a290c7d4195ac7b8b64b2d10038",
        "deepnote_cell_type": "markdown"
      },
      "source": "**NORMALIZE CONTINUOUS FEATURES (PROVIDE SOME REASONING HERE!)**",
      "block_group": "a52c055b4d1f43458131af2e8b39eb72"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVgIH7UwZ7Ry",
        "cell_id": "f71922e6453b4baba9165edc6a6947a9",
        "deepnote_cell_type": "code"
      },
      "source": "num_features = [\"coordinates_x\", \"coordinates_y\",\n                \"delta_x\", \"delta_y\",\n                \"timestamp_seconds\", \"delta_time\"]\n\n# normalize numeric features\ntrain_means = X_train_cont[num_features].mean()\ntrain_stds  = X_train_cont[num_features].std().replace(0, 1)\n\nX_train_cont_norm = (X_train_cont[num_features] - train_means) / train_stds\nX_val_cont_norm   = (X_val_cont[num_features]   - train_means) / train_stds\nX_test_cont_norm  = (X_test_cont[num_features]  - train_means) / train_stds",
      "block_group": "9b7b26c1ae6f4535a734abcd5244a828",
      "execution_count": 15,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OHYQRSkFgxz",
        "cell_id": "0191731e38174dc29d3830f9234396b3",
        "deepnote_cell_type": "markdown"
      },
      "source": "**CONVERT INTO TENSORS - EXTRACT LABELS (Y) AND WINDOW INDICES**\n\nTo allow for computations on a GPU, we need to transfrom all numpy arrays in tensors. This happens in the `make_split_tensors` function. In addition to separate tensors for categorical and continuous features, the function creates tensors for the next event labels and the indices that can be used as a starting point for the context window, which are derived from the input numpy arrays.",
      "block_group": "2cb4de6e988e4a9a84ecd0f04d736e9c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjU_oM-lFUtr",
        "cell_id": "8cec52c67e0f49ac923ae4d6b64e4f0b",
        "deepnote_cell_type": "code"
      },
      "source": "def make_split_tensors(X_events, X_coords):\n    \"\"\"\n    Splits numpy arrays and converts them into tensors that contain:\n    1) Categorical features\n    2) Continuous features\n    3) Next-event labels\n    4) Valid start indices for the context window\n    \"\"\"\n    # event labels\n    labels = X_events[\"label\"].to_numpy()\n\n    # indices where context window could be placed\n    valid_mask = X_events[\"valid_idx\"].to_numpy().astype(bool)\n\n    # remove label and valid_idx and return them reparately\n    X_events_clean = X_events.drop(columns=[\"label\", \"valid_idx\"])\n\n    # tranfrom everything into tensors\n    events_tensor = torch.tensor(X_events_clean.to_numpy(), dtype=torch.long)\n    coords_tensor = torch.tensor(X_coords.to_numpy(), dtype=torch.float32)\n    labels_tensor = torch.tensor(labels, dtype=torch.long)\n\n    # valid_idx indicates whether row in the data can be used as starting point\n    # for the context window. However, we need the actual indices (row numbers) to\n    # find them in the data set for the dataloader.\n    start_indices = torch.tensor(np.nonzero(valid_mask)[0], dtype=torch.long)\n\n    return events_tensor, coords_tensor, labels_tensor, start_indices\n\nX_train_events_tensor, X_train_coords_tensor, y_train_tensor, start_indices_train = make_split_tensors(X_train_cat, X_train_cont_norm)\nX_val_events_tensor, X_val_coords_tensor, y_val_tensor, start_indices_val = make_split_tensors(X_val_cat, X_val_cont_norm)\nX_test_events_tensor, X_test_coords_tensor, y_test_tensor, start_indices_test = make_split_tensors(X_test_cat, X_test_cont_norm)",
      "block_group": "d713d77e3fa84da3a589e870373a8271",
      "execution_count": 16,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5WGtq_IalqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def2755f-7e0b-464e-9d35-180f4741d5ba",
        "cell_id": "891321c296b741e28cb0973ddb352b47",
        "deepnote_cell_type": "code"
      },
      "source": "print(\"Train coords norm min/max:\",\n      X_train_coords_tensor.min().item(),\n      X_train_coords_tensor.max().item())",
      "block_group": "0c741e435cd0422082d9e2e326d085d9",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Train coords norm min/max: -49.00217056274414 8.97973918914795\n"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ6eYRSatU1f",
        "cell_id": "4b0ce2e0398b4018801953bcbbcbbd32",
        "deepnote_cell_type": "markdown"
      },
      "source": "### 2.1 Dataset Class and Dataloader\n\nTraining in pytorch requires a data set class that can be passed into the `DataLoader` which, in turn, implements batch sampling. The class contains methods that define the number of data samples in the data and how to sample one data example including input features and expected output.",
      "block_group": "0fcedf7934ca4ee8b0e2ccb006b57732"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olKLyZsX5nsh",
        "source_hash": "cdbf177c",
        "execution_start": 1765799012918,
        "execution_millis": 1,
        "execution_context_id": "e8669f12-f95b-4655-be6a-90a2376dcdbd",
        "cell_id": "30e81af58be44d2aaac893b5f372019d",
        "deepnote_cell_type": "code"
      },
      "source": "class FootballWindowDataset(Dataset):\n    def __init__(self, events, coords, labels, start_indices, window_size = 30):\n        # all inputs are precomputed tensors\n        self.events = events # x\n        self.coords = coords # x\n        self.labels = labels # y\n        self.start_indices = start_indices # all valid indices\n        self.window_size = window_size\n\n        # sanity check (number of rows should be equivalent)\n        assert events.shape[0] == coords.shape[0] == labels.shape[0]\n\n    def __len__(self):\n        # Number of data samples in the data set\n        # Here, this is the number of valid start indices because each sample\n        # will generate a window of window_size.\n        return self.start_indices.shape[0]\n\n    def __getitem__(self, idx):\n        # Start index needs to be obtained from the valid indices\n        # Adding the window_size gives the end index of the window\n        start = int(self.start_indices[idx])\n        end = start + self.window_size\n\n        # Extract the window from categorical features\n        x_features_cat = self.events[start:end]\n\n        # Separate categorical features\n        x_teams = x_features_cat[:, 0]\n        x_players = x_features_cat[:, 1]\n        x_events = x_features_cat[:, 2]\n\n        # Extract the window from continuous features\n        x_coords = self.coords[start:end]\n\n        # Python indexing is exclusive, meaning the end index is not included in the window.\n        # This means the correct next event label for the event that should be predicted\n        # is located at end - 1, so it is the event following the last index included in the window.\n        y = self.labels[end - 1]\n\n        return x_teams, x_players, x_events, x_coords, y",
      "block_group": "8dc260d103674be5a8b548ce8865a81a",
      "execution_count": 18,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ex7mT19tU1f",
        "cell_id": "182fcbf61f3e4fb98cf219246cd54dad",
        "deepnote_cell_type": "markdown"
      },
      "source": "## Creating DataSets and DataLoaders\nNext, we call the class on the train, validation, and test set and create the corresponding data loader. We use a `window_size` of 30 and a `batch_size` of 128. Note that we let the data loader shuffle the training indices. This randomly rearranges the starting indices for the window once per epoch before creating the batches. The data is then fed into the model in this randomised order. This is known to help with training and overfitting. Note that, despite shuffling, temporal information is retained in the context windows that are created.\nWe use a moving window here instead of a masked attention matrix because this is what seemed to work in the Simpson 2022 paper we used as inspiration. Additionally, using a window allows us to reduce memory usage, which was highly convenient, given our limited computing power. Next, a window allows to let the model learn on parts of a game, hoping that it will learn something about 'moves' in a football game.",
      "block_group": "ce9a92c4fe4f41fa80d80a82b6d195bb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XEmX5Sf5nsh",
        "source_hash": "adea9f5c",
        "execution_start": 1765799014587,
        "execution_millis": 2,
        "execution_context_id": "e8669f12-f95b-4655-be6a-90a2376dcdbd",
        "cell_id": "f7e5467249a14b4791a3cf74b7dd7b87",
        "deepnote_cell_type": "code"
      },
      "source": "num_workers = 0 # no need for workers here since we have already tensors (i think)\n\n# data sets based on FootballWindowDataset class\ntrain_dataset = FootballWindowDataset(\n    events=X_train_events_tensor,\n    coords=X_train_coords_tensor,\n    labels=y_train_tensor,\n    start_indices=start_indices_train,\n    window_size=WINDOW_SIZE)\nval_dataset = FootballWindowDataset(\n    events=X_val_events_tensor,\n    coords=X_val_coords_tensor,\n    labels=y_val_tensor,\n    start_indices=start_indices_val,\n    window_size=WINDOW_SIZE)\ntest_dataset = FootballWindowDataset(\n    events=X_test_events_tensor,\n    coords=X_test_coords_tensor,\n    labels=y_test_tensor,\n    start_indices=start_indices_test,\n    window_size=WINDOW_SIZE)\n\n# Data loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True, # shuffle for training\n    num_workers=num_workers,\n    pin_memory=True)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True)",
      "block_group": "078b4fe6519f444790ceb7f7bcb6c875",
      "execution_count": 19,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8c32AsNtU1f",
        "cell_id": "862ba11e6c3f470d9a5a8c4d5aa6c9a3",
        "deepnote_cell_type": "markdown"
      },
      "source": "The output objects of the data loader at each iterations correspond to the ouputs of the `__getitem__` method implemented in the `FootballWindowDataset` class. Each iteration in the data loader creates one batch for training/evaluating the model. So, the first dimension is of size `batch_size`. Each example in the batch contains `window_size` events that define the context, determining the size of the second dimension. For each of these `window_size` time steps, `x_teams`, `x_players`, `x_events` and `y` contain only one value, but `x_coords` comprises multiple features. Therefore, `x_coords` has a third dimension, the size of which is determined by the number of continuous features (6 in this case).\n\nThe cell below performs some sanity checks, to make sure that the dimensions of the tensors are in order. As we can see, `x_teams`, `x_players`, `x_events` and `y` are of size (`batch_size`, `window_size`), and `x_coords` is of shape (`batch_size`, `window_size`, 6).",
      "block_group": "7fdbab6f57ff406dace8011182537a4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opzKs4O1tU1f",
        "cell_id": "50d7a69a5cbb46c6bf16f5a0a2e3199c",
        "deepnote_cell_type": "markdown"
      },
      "source": "# Check whether dimensions are correct",
      "block_group": "c10619707d174d2fbc91d1e7bd694240"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "houzYrSc5nsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd18256a-01ab-47dc-b8dd-966a6e5f6e6b",
        "source_hash": "3b33c8cd",
        "execution_start": 1765799015434,
        "execution_millis": 137,
        "execution_context_id": "e8669f12-f95b-4655-be6a-90a2376dcdbd",
        "cell_id": "024bcb1091ba46bf99d3820e7cec7b24",
        "deepnote_cell_type": "code"
      },
      "source": "# Sanity checks\nx_teams, x_players, x_events, x_coords, y = next(iter(train_loader))\n\nprint(\"training set\")\nprint(\"x_teams:\", x_teams.shape)\nprint(\"x_players:\", x_players.shape)\nprint(\"x_events:\", x_events.shape)\nprint(\"x_coords:\", x_coords.shape)\nprint(\"y:\", y.shape)\n\nx_teams, x_players, x_events, x_coords, y = next(iter(val_loader))\n\nprint(\"\\nvalidation set\")\nprint(\"x_teams:\", x_teams.shape)\nprint(\"x_players:\", x_players.shape)\nprint(\"x_events:\", x_events.shape)\nprint(\"x_coords:\", x_coords.shape)\nprint(\"y:\", y.shape)",
      "block_group": "9c0faddd352e4b56ac448ae8eae5ed87",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "training set\nx_teams: torch.Size([512, 30])\nx_players: torch.Size([512, 30])\nx_events: torch.Size([512, 30])\nx_coords: torch.Size([512, 30, 6])\ny: torch.Size([512])\n\nvalidation set\nx_teams: torch.Size([512, 30])\nx_players: torch.Size([512, 30])\nx_events: torch.Size([512, 30])\nx_coords: torch.Size([512, 30, 6])\ny: torch.Size([512])\n"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFxSyCtAtU1g",
        "cell_id": "3717316ee53e4cb6af6a45e99190a08c",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 3. Methods\n\nWe chose to train three different models\n1) [Single layer baseline](#3-1-single-layer-baseline)\n2) [Transformer](#3-2-transformer)\n3) [LSTM](#3-3-lstm)\nWe discuss them in turn below.\n\n\n",
      "block_group": "5eb5f5a7823a441ba967dc8f466916f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKPh3m-UtU1g",
        "cell_id": "2b4af534f3f145df85af275eb7d6ede9",
        "deepnote_cell_type": "markdown"
      },
      "source": "### 3.1 Single Layer Baseline\nFirst, we chose to train a single layer baseline. This means we trained a network with one input and one output layer. To implement this network, we need to flatten across the time dimension. Thus, each feature (e.g., `player_id`) is split up into separate features for each time point $t$ within the context window (i.e., player_id$_t$, player_id$_{t-1}$, ..., player_id$_{t-N}$, where $N$ is the window size). Note that this discards information about relative positions of events in the temporal sequence. Additionally, the categorical features need one node for each potential class if we assume one-hot encoding per time point. Therefore, the categorical feature `player_id` requires `num_players` $\\times$ `window_size` nodes. The logic generalises to the other categorical variables. For the continuous variables, we need `num_cont_features` $\\times$ `window_size` nodes. We apply dropout to the input nodes to enforce some regularisation since this model was shown to overfit otherwise.",
      "block_group": "50888be964f641e08dca23afe07ee22c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH_CIGR6tU1g",
        "cell_id": "c84f2b5d99924f518ac985bf70d6d873",
        "deepnote_cell_type": "code"
      },
      "source": "class SingleLayerModel(nn.Module):\n  def __init__(\n      self,\n      num_features,\n      num_events,\n      num_teams,\n      num_players\n  ):\n    super().__init__()\n\n    # Dimensions\n    self.num_events = num_events\n    self.num_teams = num_teams\n    self.num_players = num_players\n\n    # Linear layer\n    self.Linear = nn.Linear(num_features, num_events)\n\n    # Dropout for regularisation\n    self.Dropout = nn.Dropout(0.1)\n\n  def forward(self, team_id, player_id, event_id, numeric):\n    B = event_id.size(0) # batch size\n\n    # Format input variables\n    # Create one-hot encodings\n    team_id_oh = F.one_hot(team_id, self.num_teams)\n    player_id_oh = F.one_hot(player_id, self.num_players)\n    event_id_oh = F.one_hot(event_id, self.num_events)\n\n    # Flatten across the window size dimension\n    # (different lags are just different features where each cat. is represented by one node)\n    team_id_oh = team_id_oh.view(B, -1) # (batch size, num_teams*window_size)\n    player_id_oh = player_id_oh.view(B, -1)\n    event_id_oh = event_id_oh.view(B, -1)\n\n    # Concatenate with numerical features\n    numeric_flat = numeric.view(B, -1) # (batch size, num_cont_features*window_size)\n\n    features = torch.cat(\n        [team_id_oh, player_id_oh, event_id_oh, numeric_flat],\n        dim=1\n    )\n\n    features_drop = self.Dropout(features)\n\n    return self.Linear(features_drop)",
      "block_group": "7e1002c60237491b846dcfc19bfd5048",
      "execution_count": 21,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w9fs4gqtU1g",
        "cell_id": "73bfad664cac46d48e4bb518f3c11a59",
        "deepnote_cell_type": "code"
      },
      "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Device to train on\nnum_features = sum([num_events, num_players, num_teams, 6]) * WINDOW_SIZE # 6 numerical variables\n\nsingle_layer_model = SingleLayerModel(num_features=num_features,\n                                      num_events=num_events,\n                                      num_teams=num_teams,\n                                      num_players=num_players).to(device)",
      "block_group": "9bda3774982b4c3a9ef4586cea21d49e",
      "execution_count": 22,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfsdRsvHtU1g",
        "cell_id": "ebe8fc4afb734b06bfb91b7fb2a82447",
        "deepnote_cell_type": "markdown"
      },
      "source": "### 3.2 Transformer\nNext, we applied a transformer. Transformers are very popular in natural language processing (NLP). Since next-word prediction seems conceptually similar to next-event prediction, we thought that the transformer might perform well in this setting too. The sequential nature of the data is comparable to language data as well.\n\nIn NLP, the transformer derives its performance largely from the underlying word embeddings. We apply a similar logic when we derive embeddings for events, players and teams. To match the continuous features in dimensions with the embeddings, we pass them through a linear layer with as many output nodes as the embedding dimension. These numbers are then summed up with positional information. Note that it might be informative where the event occurs in the sequence if, for example, some events are more likely to occur in a certain order (GOALKEEPER is likely to occur after a SHOT).\n\nThis information is fed into a `TransformerEncoderLayer` which applies the methods described in the \"Attention is All you Need\" paper (Vaswani et al., 2017). The embeddings are linearly projected to key, query and value representations. The weights for these projections are learned during training. Next, multi-headed attention is applied, where each head receives different linear projections of the input embeddings. Multi-headed attention is followed by a normalisation applied to the sum of the attention output values and the skip-connection input. Next, the values flow through two-layer feed-forward networks that are identical across the attention outputs for each event within the sequence. We chose a ReLU activation function here. The encoder layer concludes with another sum with skip connection input and normalisation. To map the final output back onto the number of event classes, we pass the output of the transformer encoder block through a fully-connected network with output layer dimension equal to `num_events`.\n\nWe discovered that the transformer quickly improved during model training, so we made it as simple as possible without decreasing its performance on the validation set in order to enforce the principle of parsimony. We ended up with the following hyperparameters: An embedding dimension of four, one attention head, 10\\% drop-out (for regularisation) and an expanded dimension of eight in the position-wise feed-forward networks, only one transformer block and one fully-connected layer with 10\\% dropout to arrive at the logits for the event classes.",
      "block_group": "0b0e068a43a144bbb3a18fdb9269aa90"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL3seUNt5nsh",
        "source_hash": "98e81972",
        "execution_start": 1765799017424,
        "execution_millis": 1,
        "execution_context_id": "e8669f12-f95b-4655-be6a-90a2376dcdbd",
        "cell_id": "b1f394e516a24f478cfb1f2a0a65a343",
        "deepnote_cell_type": "code"
      },
      "source": "class FootballTransformer(nn.Module):\n    def __init__(self, num_events, num_players, num_teams,\n                 d_model=4, num_features=6, nhead=1,\n                 window_size=30):\n        super().__init__()\n\n        self.event_emb  = nn.Embedding(num_events, d_model) # event embeddings\n        self.player_emb = nn.Embedding(num_players, d_model) # player embeddings\n        self.team_emb   = nn.Embedding(num_teams, d_model) # team embeddings\n        self.num_linear = nn.Linear(num_features, d_model) # feed-forward net for continuous features\n\n        # Encodes the position in the sequence\n        self.pos_emb = nn.Embedding(window_size, d_model) # positional encodings\n\n        # Encoder layer for transformer\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, # Embedding dimension\n            nhead=nhead, # One head\n            dim_feedforward=8, # Dimension of position-wise feedforward net\n            activation='relu', # ReLU activation function\n            dropout=0.1, # 10% dropout\n            batch_first=True) # Tells the model to make batch size first dimension\n\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1) # One transformer block\n\n        # Fully-connected feed-forward network to map onto the event classes\n        self.fc = nn.Linear(d_model, num_events)\n\n        # Dropout for feed-forward network\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, team_id, player_id, event_id, numeric):\n        B, L = event_id.shape # (batch_size, window_size)\n\n        # Embeddings\n        e = self.event_emb(event_id)\n        p = self.player_emb(player_id)\n        t = self.team_emb(team_id)\n        n = self.num_linear(numeric)\n\n        # Add all of the above\n        x = e + p + t + n\n\n        # Positional encoding\n        # Create range along window_size\n        positions = torch.arange(L, device=event_id.device).unsqueeze(0)\n        pos = self.pos_emb(positions) # learn position encodings\n        x = x + pos # add to all other features/feature embeddings\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        # Grab the last timestep since we are only interested in the\n        # next-event prediction at the end of the context window\n        last_h = x[:, -1, :]\n        last_h = self.dropout(last_h)\n\n        logits = self.fc(last_h) # logits for each of the event classes\n\n        return logits",
      "block_group": "953d11e36a634be0b9eaf2d31d6330b7",
      "execution_count": 23,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D5B25rmtU1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6f80aa-608f-45c2-e324-50bf11370e15",
        "cell_id": "08a28a0c4fc349d08db1c077059d1df0",
        "deepnote_cell_type": "code"
      },
      "source": "transformer_model = FootballTransformer(\n    num_events=num_events,\n    num_players=num_players,\n    num_teams=num_teams,\n    d_model=TRANSFORMER_DIMENSION,\n    window_size=WINDOW_SIZE).to(device)",
      "block_group": "4bc05c1e19b9464ebf2fd6655b6bf3f8",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n  warnings.warn(\n"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4tuV8cX8-ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f15d24d-59d6-4294-ae90-f729b71ee630",
        "cell_id": "3ba2c290549442e39fc4f6fdb50acd98",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "block_group": "b0c73306be56434f93d048052eb934e9",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "27"
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HvO_TzXtU1g",
        "cell_id": "b088d5f9aa174f95ad6f089544afdc80",
        "deepnote_cell_type": "markdown"
      },
      "source": "### 3.3 LSTM",
      "block_group": "f7908a5b5dcd41e387dbeb87d5e97d2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jCZlqOTtU1g",
        "cell_id": "bedabc64b95542328def879eaa405c1d",
        "deepnote_cell_type": "markdown"
      },
      "source": "We decided to use an LSTM model as a comparison to the transformer model because we expected it to perform worse, given that the LSTM does not perform as well with long-range dependencies compared to a transformer model. This is a minimal comparison, in that everything except the actual model cell stays the same (i.e. categorical and numerical features are fed through the same embedding layers as in the transformer model. Both models employ the same classification head) Regarding the architecture of the LSTM: It consists of 2 LSTM stacked LSTM cells and has a hidden dimension of 250. All other modelling choices stay exactly the same compared to the transformer model.",
      "block_group": "2e34f8920fda49c687ab0f7fdaf495f8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7or6sVQXtU1g",
        "cell_id": "edc850f8552b4fb987c8effad5c91219",
        "deepnote_cell_type": "code"
      },
      "source": "# LSTM\nclass LSTM_MODEL(nn.Module):\n    def __init__(self, hidden_dim,\n                 layer_dim, num_events,\n                 num_players, num_teams, d_model=4,\n                 num_features = 6,\n                 ):\n\n        super().__init__()\n\n        self.event_emb  = nn.Embedding(num_events, d_model) # Event embeddings\n        self.player_emb = nn.Embedding(num_players, d_model) # Player embeddings\n        self.team_emb   = nn.Embedding(num_teams, d_model) # Team embeddings\n        self.num_linear = nn.Linear(num_features, d_model)  # Feed-forward net for continuous features\n\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim # Nr of cells stacked behind each other\n        self.lstm = nn.LSTM(d_model, hidden_dim, layer_dim, batch_first=True)\n        self.fc1 = nn.Linear(hidden_dim, num_events)\n        self.dropout = nn.Dropout(p = 0.1)\n\n    # Exactly the same logic compared to the transformer\n    def forward(self, team_id, player_id, event_id, numeric):\n        B, L = event_id.shape\n\n        # embeddings\n        e = self.event_emb(event_id)\n        p = self.player_emb(player_id)\n        t = self.team_emb(team_id)\n        n = self.num_linear(numeric)\n\n        x = e + p + t + n\n\n        x = self.dropout(x)\n\n        out, (h_n, c_n) = self.lstm(x)\n\n        last_h = h_n[-1]\n\n        last_h = self.dropout(last_h)\n        logits = self.fc1(last_h)\n        # h = F.relu(self.fc1(last_h))\n        # h = self.dropout(h)\n        # logits = self.fc2(h)\n        return logits",
      "block_group": "865c2fc7af4649ad8028d39431dc5f16",
      "execution_count": 26,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKraX81gtU1g",
        "cell_id": "6c2d5c05823b47be87d1b7793d6bf9a5",
        "deepnote_cell_type": "code"
      },
      "source": "lstm_model = LSTM_MODEL(\n    num_events=num_events,\n    num_players=num_players,\n    num_teams=num_teams,\n    hidden_dim = 250, layer_dim = 2).to(device)",
      "block_group": "6e974e0904cc44908c398e756febc00f",
      "execution_count": 27,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdgQ-b02tU1g",
        "cell_id": "0810db8210a44081b1019946388aa8b9",
        "deepnote_cell_type": "markdown"
      },
      "source": "**Training Parameters:** Here we initialize the parameters for training and specify to use automatic mixed precision to speed up training by dynamically using as much floating point precision as necessary. Cross entropy is used as the loss function because we are trying to predict categorical targets. We used the Adam algorith since it is the one we were introduced to in the course. We added a scheduler to help reduce the learning rate in case the gradient decent algorithm gets stuck. The scheduler basically waits for a pre-specified number of epochs (patience) and if there is not change in the validatio loss between those epoch it reduces the learning rate by a factor.",
      "block_group": "81581bb9d61d4d6cb25f685c7253b1be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J9aJTqM5nsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa43fd61-a660-4d49-dedd-bb401964737a",
        "source_hash": "d9b752c9",
        "execution_start": 1765799050159,
        "execution_millis": 419,
        "execution_context_id": "e8669f12-f95b-4655-be6a-90a2376dcdbd",
        "cell_id": "d9135e79864d4f428c217c137a750722",
        "deepnote_cell_type": "code"
      },
      "source": "scaler = GradScaler() # for mixed precision\nuse_amp = (device.type == \"cuda\") # automatic mixed precision if model on GPU\ncriterion = torch.nn.CrossEntropyLoss()\n# Transformer\noptimizer_transformer = torch.optim.AdamW(transformer_model.parameters(), lr=LEARNING_RATE)\nscheduler_transformer = ReduceLROnPlateau(optimizer_transformer, factor=0.1, patience=PATIENCE)\n# LSTM\noptimizer_lstm = torch.optim.AdamW(lstm_model.parameters(), lr=LEARNING_RATE)\nscheduler_lstm = ReduceLROnPlateau(optimizer_lstm, factor=0.1, patience=PATIENCE)\n\nprint(next(transformer_model.parameters()).device) # check if model is on gpu",
      "block_group": "59c0babf9ead4dad819f1c485f1b504d",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "cuda:0\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipython-input-3043898269.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler() # for mixed precision\n"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYVbiJittU1g",
        "cell_id": "176e374057f442cbb3a4d4d4402e905b",
        "deepnote_cell_type": "markdown"
      },
      "source": "### Training Functions\nThe following functions will all be used to train and evaluate the model over multiple epochs. `one_epoch` is used to either train or evaluate the model. In training mode, accuracy and loss per batch will be printed. In evaluation mode, it will additionally create confusion matrices.",
      "block_group": "8267c08816394621bd3b907a011ce579"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHx5s1QStU1h",
        "cell_id": "f43a7c1f29ff4b81a12c7c73faf5762d",
        "deepnote_cell_type": "code"
      },
      "source": "def one_epoch(model, dataloader, optimizer, epoch_idx: int, use_amp: bool = False, train = False, conf_mat = None):\n    model.train() if train else model.eval()\n\n    if not train:\n        # initialise confusion matrix on first epoch\n        if conf_mat == None:\n            conf_mat = ConfusionMatrix(task='multiclass', num_classes=num_events).to(device) # rows are true, columns predicted classes\n        else:\n            conf_mat.reset()\n\n    total_loss = 0.0\n    total_correct = 0.0\n    total_examples = 0.0\n\n    with torch.set_grad_enabled(train):\n        for batch_idx, batch in enumerate(dataloader):\n            team_ids, player_ids, event_ids, numeric, target = batch\n\n            team_ids   = team_ids.to(device, non_blocking=True).long() # Send to GPU\n            player_ids = player_ids.to(device, non_blocking=True).long() # Send to GPU\n            event_ids  = event_ids.to(device, non_blocking=True).long() # Send to GPU\n            numeric    = numeric.to(device, non_blocking=True).float() # Send to GPU\n            target     = target.to(device, non_blocking=True).long() # Send to GPU\n\n            if train:\n                optimizer.zero_grad()\n            # forward (optionally with AMP, but no backward)\n            if use_amp and device.type == \"cuda\": # Only use amp when on GPU\n                with torch.amp.autocast(\"cuda\"):\n                    logits = model(team_ids, player_ids, event_ids, numeric) # Forward\n                    loss = criterion(logits, target)\n                if train:\n                    scaler.scale(loss).backward() # Scales loss to use AMP and backprop\n                    scaler.step(optimizer) # Replaces optimizer.step\n                    scaler.update()\n            else:\n                # Same logic but no GPU and no AMP\n                logits = model(team_ids, player_ids, event_ids, numeric)\n                loss = criterion(logits, target)\n                if train:\n                    loss.backward()\n                    optimizer.step()\n\n            batch_size_cur = target.size(0) # Examples in batch\n            total_loss += loss.item() * batch_size_cur # Loss at current iteration\n            total_examples += batch_size_cur # Add batch size to total seen events\n\n            # keep track of predictions & accuracy for conf matrix\n            preds = logits.argmax(dim=-1)\n            total_correct += (preds == target).sum().item()\n\n\n            if train:\n                if (batch_idx + 1) % 1000 == 0: # Every 1000 matches\n                    batch_loss = total_loss / total_examples # Avg loss so far\n                    print(f\"  [EPOCH {epoch_idx+1}, BATCH {batch_idx+1}] avg train loss: {batch_loss:.4f}\")\n                    print(\"========== GRADIENTS ==========\")\n                    for name, p in model.named_parameters():\n                        if p.grad is not None:\n                            print(name, p.grad.norm())\n            else:\n                 # per-batch logging similar to training\n                if (batch_idx + 1) % 1000 == 0:\n                    batch_loss = total_loss / total_examples\n                    batcg_acc  = total_correct / total_examples\n                    print(f\"  [epoch {epoch_idx+1}, batch {batch_idx+1}] \"\n                        f\"avg val loss: {batch_loss:.4f}, \"\n                        f\"avg val acc: {batcg_acc:.4f}\")\n                # Update the confusion matrix\n                conf_mat.update(preds.flatten(), target.flatten())\n\n        # Average loss per Val\n        epoch_loss = total_loss / total_examples\n        accuracy = total_correct / total_examples\n\n    return epoch_loss, accuracy, conf_mat",
      "block_group": "d55ab65c8cb94a2ba9ea872e7515da78",
      "execution_count": 29,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k2WU_uXdx7P",
        "cell_id": "93a21c066c5247cab5f21813327f0e49",
        "deepnote_cell_type": "code"
      },
      "source": "def plot_curves(train_losses, val_losses, val_accuracies):\n    clear_output(wait=True)  # clears previous output to make plotting smoother\n\n    epochs_range = range(1, len(train_losses) + 1) # x-axis\n\n    fig, axes = plt.subplots(2, 1, figsize=(5, 4), sharex=True) # Init the plots\n\n    # Plot train and val losses\n    ax = axes[0] # Select top plot\n    ax.plot(epochs_range, train_losses, # epochs on x, training loss on y\n            color=\"black\", marker=\"o\", linestyle=\"-\", label=\"Train loss\")\n    ax.plot(epochs_range, val_losses,\n            color=\"red\", marker=\"o\", linestyle=\"-\", label=\"Val loss\")\n    ax.set_ylabel(\"Loss\")\n\n    # Set dyncamic y axis limits\n    all_losses = np.array(train_losses + val_losses)\n    if all_losses.size > 0:\n        lo = all_losses.min()\n        hi = all_losses.max()\n        margin = 0.1 * (hi - lo + 1e-4)\n        ax.set_ylim(lo - margin, hi + margin)\n\n    ax.set_title(\"Training & validation loss\", fontsize=10)\n    ax.legend()\n    ax.grid(True)\n    ax.set_axisbelow(True)\n\n    # Same logica as above but now for accuracies\n    ax = axes[1]\n    ax.plot(epochs_range, val_accuracies,\n            color=\"red\", marker=\"o\", linestyle=\"-\", label=\"Val accuracy\")\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Accuracy\")\n\n    accs = np.array(val_accuracies)\n    if accs.size > 0:\n        lo = accs.min()\n        hi = accs.max()\n        margin = 0.1 * (hi - lo + 1e-4)\n        ax.set_ylim(lo - margin, hi + margin)\n\n    ax.set_title(\"Validation accuracy\", fontsize=10)\n    ax.legend()\n    ax.grid(True)\n    ax.set_axisbelow(True)\n\n    plt.tight_layout()\n    plt.show()",
      "block_group": "0558daeca76740b68be562bb7b8075c2",
      "execution_count": 30,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf3suECntU1h",
        "cell_id": "27f969a2c5754b288f0998b48c9ee55f",
        "deepnote_cell_type": "markdown"
      },
      "source": "### Checkpoints\nWe included the feature for training to save all model states at each epoch, as well as the currently best performing version of the model directly to the harddrive. This is because training would sometimes end, when runtimes are cancelled, laptops closed etc. Now, we don't have to worry about losing all our progress. The paths to the folders, where you want history and checkpoints saved have to be manually put it. Checkpoints and history is saved, attached with a timestamp to keep track from when this model version is. To run this, patch folders have to be specified beforehand. If none are specified, the current working directory will be used",
      "block_group": "19c4941a305c4138a4fea943804711fc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlcBo-w6tU1h",
        "cell_id": "71cc1fe7280c47fcb8f13213e011b023",
        "deepnote_cell_type": "code"
      },
      "source": "# To save intermediate weights & history\nroot_path = os.getcwd() # Specify where you want this to be saved\n\nsave_ckpt = os.path.join(root_path, \"checkpoints\")\nos.makedirs(save_ckpt, exist_ok=True) # create the folder if doesn't exist\n\nsave_hist = os.path.join(root_path, \"histories/\")\nos.makedirs(save_hist, exist_ok=True)\n\nlast_ckpt_path = os.path.join(save_ckpt, \"model_last\")\nos.makedirs(last_ckpt_path, exist_ok=True)\nbest_ckpt_path = os.path.join(save_ckpt, \"model_best\")\nos.makedirs(best_ckpt_path, exist_ok=True)",
      "block_group": "324b11dd3f574cbaac52514f780bfed2",
      "execution_count": 49,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5csudUSuraQ6",
        "cell_id": "3aa33921120f430e849f69ae8afaf820",
        "deepnote_cell_type": "code"
      },
      "source": "def save_checkpoint(path, epoch, model, optimizer, scheduler, best_val_loss=None):\n    \"\"\" Helper function to save the current model state into a python dictionary and onto the harddrive\"\"\"\n    checkpoint = {\n        \"epoch\": epoch,\n        \"model_state\": model.state_dict(),\n        \"optimizer_state\": optimizer.state_dict(),\n        \"scheduler_state\": scheduler.state_dict() if scheduler is not None else None,\n        \"best_val_loss\": best_val_loss,\n    }\n    torch.save(checkpoint, path)\n    print(f\"Saved checkpoint to: {path}\")\n\ndef load_checkpoint(path, model, optimizer, scheduler, device):\n    \"\"\" Function to load a previous training state of a model\"\"\"\n\n    ckpt = torch.load(path, map_location=device) # Load the model dictionary\n\n    # Get all training states\n    model.load_state_dict(ckpt[\"model_state\"])\n    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n    if ckpt.get(\"scheduler_state\") is not None and scheduler is not None:\n        scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n\n    # New start epoch\n    start_epoch = ckpt[\"epoch\"] + 1\n    best_val_loss = ckpt.get(\"best_val_loss\", float(\"inf\"))\n\n    print(f\"Resuming training from epoch {start_epoch}, best_val_loss={best_val_loss:.4f}\")\n    return start_epoch, best_val_loss\n\ndef get_last_checkpoint(ckpt_dir, ext=\".pt\"):\n    \"\"\"Return path to newest checkpoint file in a directory, or None if empty.\"\"\"\n    ckpt_dir = os.path.abspath(ckpt_dir)\n    files = [\n        os.path.join(ckpt_dir, f)\n        for f in os.listdir(ckpt_dir)\n        if f.endswith(ext)\n    ]\n    if not files:\n        return None\n    return max(files, key=os.path.getmtime)",
      "block_group": "c868c88c57444b9085de6ad062e04fb5",
      "execution_count": 32,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGILDmwFtU1h",
        "cell_id": "17670b35814041d4b0a5b3abdb8dee31",
        "deepnote_cell_type": "markdown"
      },
      "source": "### The Scheduler\nFollowing the methodology of Simpson et al. (2022), we decided to include the `scheduler` for two reasons. Firstly, after training the model for a couple of epochs, we noticed that the model improves very little after three epochs. We then first tried out a couple of learning rates to see what changes and then decided to insert the `scheduler`, which is able to dynamically update the learning rate based on the loss per epoch. Second, the changing of learning rates during training allows to build an early stopping mechanism, when the learning rate becomes too low. We decided to use 0.0000001 as the lowest allowed learning rate. The `scheduler` works by adjusting the learning rate after some number of epochs has passed. We decided on `Patience` = 2 which seemed to work fine during training.",
      "block_group": "0c3420efc8dd48b79f3928bdba4f7b56"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn9b7sEYtU1h",
        "cell_id": "07c6ac7668a842be9bf4679bf5f5c496",
        "deepnote_cell_type": "markdown"
      },
      "source": "The following code cell uses the previously defined functions to train and evaluate the model for one epoch at a time. This can optionally be used with a previously saved checkpoint to start training from partially trained weights.",
      "block_group": "d6097c72a39442a794c459d9e49bb23d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj935Dvz3JfN",
        "colab": {
          "height": 35,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab0094d-7626-4015-e6ab-40050c9f4a4f",
        "cell_id": "66068f17fbc54c78b6125ea015a34888",
        "deepnote_cell_type": "code"
      },
      "source": "transformer_model.__class__.__name__",
      "block_group": "7020024e090e4ed5a623b30738a72174",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'FootballTransformer'",
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClFwUM1uxrBr",
        "cell_id": "df3bcf6bcc4f43dc9fa4470b52e4b3c9",
        "deepnote_cell_type": "code"
      },
      "source": "def train_model(model, num_epochs, optimizer, scheduler, hist_path, resume_ckpt=None, use_amp= True):\n  \"\"\"Main training function, which incorporates all previous functions.\n     Training is possible to start from checkpoint. All history and checkpoints are saved to disk\"\"\"\n\n  # Optional resume training from saved checkpoint\n  if resume_ckpt is not None and os.path.isfile(resume_ckpt):\n    start_epoch, best_val_loss = load_checkpoint(path=resume_ckpt,\n                                                 model=model,\n                                                 optimizer=optimizer,\n                                                 scheduler=scheduler,\n                                                 device=device)\n  else:\n    start_epoch = 0\n    best_val_loss = float(\"inf\")\n    print(\"Starting training...\")\n\n  # Training states\n  history = {\"train_loss\": [],\n             \"val_loss\": [],\n             \"val_acc\": [],\n             \"conf_mat\": None}\n\n  for epoch in range(start_epoch, num_epochs): # If start from checkpoint num_epochs: Additional epochs you want to train\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    # training\n    print(\"=== TRAINING ===\")\n    start_train = time.time() # Check how long training one epoch took\n    train_loss, _ , _ = one_epoch(model, train_loader, optimizer = optimizer, epoch_idx = epoch, use_amp = use_amp, train = True) # Actual training\n    if device.type == \"cuda\":\n        torch.cuda.synchronize() # Blocks CPU\n    train_time = time.time() - start_train\n\n    # validation - Same logic as above\n    print(\"=== VALIDATION ===\")\n    start_val = time.time()\n    val_loss, val_acc, val_conf_mat = one_epoch(model, val_loader, optimizer = optimizer, conf_mat=history[\"conf_mat\"], epoch_idx=epoch, use_amp=use_amp, train = False)\n    if device.type == \"cuda\":\n        torch.cuda.synchronize()\n    val_time = time.time() - start_val\n\n    # bookkeeping\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n    history[\"conf_mat\"] = val_conf_mat\n\n    torch.save(history, hist_path + model.__class__.__name__ + f\"_history_epoch{epoch+1}.pt\") # save history externally\n\n    # Calling the plotting function\n    plot_curves(history[\"train_loss\"],\n                history[\"val_loss\"],\n                history[\"val_acc\"])\n\n    print(f\"Epoch {epoch+1} summary:\")\n    print(f\"  Total epoch time: {train_time + val_time:.2f}s\")\n    print(f\"  Train loss: {train_loss:.4f} (time: {train_time:.2f}s)\")\n    print(f\"  Val   loss: {val_loss:.4f}, Val acc: {val_acc:.4f} (time: {val_time:.2f}s)\")\n\n    # Save checkpoint\n    current_time = time.gmtime()\n    current_time = list(current_time[1:5]) # Select the info we need\n    # Time saved in month-year_hour:minute\n    current_time_selected = str(current_time[1]) + '-' + str(current_time[0]) +  '_' + str(current_time[2]) + ':' + str(current_time[3])\n\n    # Save the files based on time of training\n    path = os.path.join(last_ckpt_path, model.__class__.__name__ + '_' + current_time_selected + f'_epoch{epoch+1}.pt')\n\n    # Save to disk\n    save_checkpoint(\n        path=path,\n        epoch=epoch,\n        model=model,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        best_val_loss=best_val_loss)\n\n    # Save new best checkpoint\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n\n        # Same logic as above\n        current_time = time.gmtime()\n        current_time = list(current_time[1:5])\n        current_time_selected = str(current_time[1]) + '-' + str(current_time[0]) +  '_' + str(current_time[2]) + ':' + str(current_time[3])\n\n        path = os.path.join(best_ckpt_path, model.__class__.__name__ + '.pt')\n\n        # Saving to disk\n        save_checkpoint(\n            path=path,\n            epoch=epoch,\n            model=model,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            best_val_loss=best_val_loss,\n        )\n        print(\"New best model based on validation loss\")\n\n    # Scheduler able to adapt the learning rate based on validation loss\n    scheduler.step(val_loss)\n    current_lr = optimizer.param_groups[0][\"lr\"]\n    print(f\"  LR now: {current_lr:.6e}\")\n\n    # Pls sir stop training, my learning rate is too small\n    if current_lr < 1e-7:\n        print(\"Threshold lr reached. Stopping early!\")\n        break\n\n  print(\"\\nTraining finished.\")\n  print(f\"Best validation loss seen: {best_val_loss:.4f}\")\n  print(f\"Best checkpoint saved at:  {best_ckpt_path}\")\n  print(f\"Last checkpoint saved at:  {last_ckpt_path}\")\n\n  return history",
      "block_group": "5fc2a89cdced4e96913d999a058448d0",
      "execution_count": 37,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHz_4DnStU1h",
        "cell_id": "a9386d4256d34c3fb7cf15c9f5f2addc",
        "deepnote_cell_type": "markdown"
      },
      "source": "## Model Training\nFinally, we can train our model. As seen in the functions above, all process during all epochs will be saved in the prespecified folders. After each training epoch, a validation set will be used to check the model performance. In our project the model was trained on 50 epochs. After each epoch, the plot will display the current training and validation accuracies, so we can keep track of model performance in real-time.",
      "block_group": "5d6d5ff2cfdb484dace0e9c31e2204fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWt8kMv5tU1h",
        "cell_id": "db3a96717fe5430aa213411f79fbce5f",
        "deepnote_cell_type": "markdown"
      },
      "source": "Please note that because of the logic, implemented in the training functions, if a checkpoint is called `num_epochs` is not the additional Nr of epochs to be trained, but the target epoch. (e.g. if a model has been trained for 5 epochs and you want an additional 5, `num_epochs` = 10)",
      "block_group": "354013ca744a497d9b76d9ffe43e48ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJFnxyeghycf",
        "cell_id": "1b5eaabf53b84410bdebe0289617aa36",
        "deepnote_cell_type": "code"
      },
      "source": "num_epochs = 1",
      "block_group": "f377818d7c0546f9ba8231a81bcb1603",
      "execution_count": 72,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LJ93ShCtU1i",
        "cell_id": "098c47b3a2814157bcd5c35da3cc9794",
        "deepnote_cell_type": "markdown"
      },
      "source": "#### Run this cell if you want to train from the start",
      "block_group": "a87edae7795a4880857387e1fcfc33b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0yhk-VY1nil",
        "colab": {
          "height": 639,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d8ac81-1341-43a9-e213-5ed510e14fed",
        "cell_id": "7216d126df5f4b35a506245024f30566",
        "deepnote_cell_type": "code"
      },
      "source": "# Train from start\nhistory_transformer = train_model(\n    model = transformer_model,\n    optimizer = optimizer_transformer,\n    scheduler = scheduler_transformer,\n    num_epochs=num_epochs,\n    hist_path=save_hist,\n    resume_ckpt=None,\n    use_amp=use_amp)",
      "block_group": "821f2f6b27094497928d26c47ef80b27",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 500x400 with 2 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1 summary:\n  Total epoch time: 106.17s\n  Train loss: 1.4193 (time: 85.06s)\n  Val   loss: 1.3899, Val acc: 0.5971 (time: 21.11s)\nSaved checkpoint to: /content/checkpoints/model_last/FootballTransformer_20-12_22:42_epoch1.pt\nSaved checkpoint to: /content/checkpoints/model_best/FootballTransformer.pt\nNew best model based on validation loss\n  LR now: 1.000000e-04\n\nTraining finished.\nBest validation loss seen: 1.3899\nBest checkpoint saved at:  /content/checkpoints/model_best\nLast checkpoint saved at:  /content/checkpoints/model_last\n"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEP0g247tU1i",
        "cell_id": "626b68b4eb7e4d8e836d892db429259f",
        "deepnote_cell_type": "markdown"
      },
      "source": "#### Run this cell if you want to train from a checkpoint\nIf training was interrupted as some point, or the model should be trained for longer, please continue training using the cells below.",
      "block_group": "b25d3bbe21c04f8481f03b7e51ed778b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mzcFvSt1ury",
        "cell_id": "4dd46a4f2dae4826a822d1e7170fe065",
        "deepnote_cell_type": "code"
      },
      "source": "# train from checkpoint\n# re-initialize because google colab :/\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=num_workers,\n    pin_memory=True)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True)\nhistory = train_model(num_epochs=num_epochs,\n                      hist_path=save_hist,\n                      resume_ckpt=last_ckpt_path,\n                      use_amp=use_amp)",
      "block_group": "1c5c5e40d382432ca7f789935a79c68e",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZJ4OiQetU1i",
        "cell_id": "f96eabc79b5b404685730136f2d58315",
        "deepnote_cell_type": "markdown"
      },
      "source": "## Part 4: Results",
      "block_group": "236cb5d4bbfe444fa7c28818edfdede9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOQOUceptU1i",
        "cell_id": "dbd8d45a8bd04ec5babf200a3356504e",
        "deepnote_cell_type": "markdown"
      },
      "source": "Here we are going to present the results of this project. That is, our main variable of interest is model prediction performance, measured via the test set accuracies. All models will be compared on this metric. All models have been trained on the same training set, and validation set performance has been tracked during training. Similarly, the same test set is going to be used to compare model performance. The model state being used, is going to be the instance where each model achieved the highest validation accuracy. Additionally, we included confusion matrices to gain more insights into what kind of mistakes the models make and how they differ from each other.",
      "block_group": "9c9f71e4718e44849e07ebded745d58a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94-DOg8s9Hdz",
        "cell_id": "9fbd785e16eb4abeac85375fd6f39721",
        "deepnote_cell_type": "markdown"
      },
      "source": "**TEST SET ACCURACY**",
      "block_group": "14767df7cd644926aacfcf2f572ed57f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUR8scT7tU1i",
        "cell_id": "fa20a9deaa0141318af4a5397921f9f2",
        "deepnote_cell_type": "code"
      },
      "source": "\ndef test_model(model, test_loader, hist_path, optimizer, best_ckpt_path = best_ckpt_path, suppress_print = False):\n    \"\"\"Convenience function to get the test performance and immediately print it\"\"\"\n\n    # Select the model with the highest performance\n    ckpt_path = os.path.join(best_ckpt_path, model.__class__.__name__ + \".pt\")\n    ckpt = torch.load(ckpt_path, map_location=device)\n    model.load_state_dict(ckpt[\"model_state\"])\n    print(\"Loaded best checkpoint from:\", best_ckpt_path)\n    print(\"Best val loss stored in ckpt:\", ckpt.get(\"best_val_loss\", \"N/A\"))\n\n    test_loss, test_acc, conf_mat = one_epoch(model = model, optimizer=optimizer, dataloader=test_loader, epoch_idx=0, use_amp=use_amp, train = False)\n\n    if not suppress_print:\n        print(\"\\n=== TEST RESULTS ===\")\n        print(f\"Test loss: {test_loss:.4f}\")\n        print(f\"Test acc : {test_acc:.4f}\")\n\n    save_hist = hist_path + model.__class__.__name__ + f\"_history_epoch{num_epochs}.pt\"\n    history = torch.load(save_hist, weights_only = False)\n    last_train_loss = history[\"train_loss\"][-1]\n    last_val_loss = history[\"val_loss\"][-1]\n    last_val_acc = history[\"val_acc\"][-1]\n\n    if not suppress_print:\n        print(\"\\n=== VALIDATION RESULTS ===\")\n        print(f\"Validation loss: {last_val_loss:.4f}\")\n        print(f\"Validation acc : {last_val_acc:.4f}\")\n\n    return test_loss, test_acc, last_val_acc, conf_mat\n",
      "block_group": "c891d5a124324a859d3b70a6f930698c",
      "execution_count": 77,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpitKXCmtU1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3661740-1f9b-4a1b-dd7f-4d45891f7bb5",
        "cell_id": "09bf8730a8d9448d9b11ce66c397f121",
        "deepnote_cell_type": "code"
      },
      "source": "test_loss, test_acc, last_val_acc, conf_mat = test_model(\n    transformer_model,\n    hist_path = save_hist,\n    optimizer = optimizer_transformer,\n    best_ckpt_path = best_ckpt_path,\n    test_loader = test_loader\n)\n",
      "block_group": "3195ee49cf6f47158f1f5582bf90dd5c",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loaded best checkpoint from: /content/checkpoints/model_best\nBest val loss stored in ckpt: 1.3898718088030209\n\n=== TEST RESULTS ===\nTest loss: 1.3807\nTest acc : 0.6002\n\n=== VALIDATION RESULTS ===\nValidation loss: 1.3899\nValidation acc : 0.5971\n"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSbfSOVjG7kS",
        "cell_id": "f9c65994f31a4a329b9951a1c5855d88",
        "deepnote_cell_type": "markdown"
      },
      "source": "**ACCURACY PLOTS**",
      "block_group": "139f9ff00afb48328c8622d66d0b4419"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNbBL5YKAXrb",
        "colab": {
          "height": 306,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a4fe60-eeb0-46e9-de97-6f81cb03cc1e",
        "cell_id": "fe3b9b278b7a463c808bd13c1a299b3c",
        "deepnote_cell_type": "code"
      },
      "source": "labels = [\"Val\", \"Test\"]\nvalues = [last_val_acc, test_acc]\nx = np.arange(len(labels))\n\nplt.figure(figsize=(4, 3))\nplt.bar(x, values, color=[\"red\", \"blue\"])\nplt.xticks(x, labels)\nplt.ylabel(\"Accuracy\")\nplt.ylim(0, max(values) + 1e-2)\nplt.grid(axis=\"y\", alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
      "block_group": "96553168372a4b64a34d241389d98b09",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 400x300 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923zOIUoGy3Q",
        "cell_id": "84992246f76a4e8389265530c04f5533",
        "deepnote_cell_type": "markdown"
      },
      "source": "**MAJORITY CLASS AND UNIFORM PROBABILITIES**",
      "block_group": "919d65f7d73147aa8f5b2237b318ae97"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np4XAm5eEHzs",
        "colab": {
          "height": 365,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d381052b-c3c0-44d8-f8ea-c7330f98ab2a",
        "cell_id": "de423431f38b4541a850fc6df8753b44",
        "deepnote_cell_type": "code"
      },
      "source": "history = torch.load(save_hist)\nX_test_desc = X_test[X_test[\"valid_idx\"] == 1].copy()\n\n\nlabel_counts = X_test_desc[\"label\"].value_counts()\nlabel_probs  = X_test_desc[\"label\"].value_counts(normalize=True)\nnum_classes = label_counts.shape[0]\n\nprint(\"Num classes in test:\", num_classes)\nprint(\"Top 10 labels (test):\")\nprint(label_counts.head(5))\nprint(\"\\nTop 10 label probs (test):\")\nprint(label_probs.head(5))\n\nprint(\"\\nMajority-class probability:\", label_probs.iloc[0])\nprint(\"Uniform probability:\", 1/num_classes)\nprint(\"Model accuracy:\", history[\"val_acc\"][-1])\n",
      "block_group": "a229718994ac41f794e0024b70cac7e5",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/histories/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3100867190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid_idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/histories/'"
          ]
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDCkF-O-tU1i",
        "cell_id": "006ef9d31dea4951a74f44dd9399021d",
        "deepnote_cell_type": "markdown"
      },
      "source": "#### Confusion Matrix",
      "block_group": "63a25258bfc5411f99918e65d32dea1e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ija0rrtU1i",
        "cell_id": "21b7b9145a64490fbd6176d36640d83d",
        "deepnote_cell_type": "code"
      },
      "source": "def plot_conf_mat(history):\n    \"\"\"\n    Function takes a history dictionary from training as input.\n    Plots the confusion matrix as a heatmap.\n    \"\"\"\n    # Compute confusion matrix, move it to CPU and convert to numpy array\n    val_conf_mat = history[\"conf_mat\"].compute().cpu().numpy()\n\n    # Get event names in correct order\n    event_names = codes_map[\"input_event\"]\n\n    # Turn confusion matrix into pandas dataframe\n    val_cm_df = pd.DataFrame(val_conf_mat, index=event_names, columns=event_names)\n\n    # Divide each colum entry by the row sum to get P(pred|target)\n    cm_prob = val_cm_df.div(val_cm_df.sum(axis=1), axis=0) # rows are nomalised now\n\n    # The GENERIC:generic class contains events that do not fall into the other classes.\n    # Let's give it a more intuitive name.\n    # The conversion to probabilities within rows led to NAs because <BOM> is never predicted\n    # since the context window can never span two matches. We remove the NAs.\n    cm_prob = cm_prob.rename(\n        columns={\"GENERIC:generic\": \"OTHER\"},\n        index={\"GENERIC:generic\": \"OTHER\"}).dropna(axis=0)\n\n    # Plot the confusion matrix as a heatmap\n    cmap = sns.color_palette(\"flare\", as_cmap=True)\n    plt.figure()\n    ax = sns.heatmap(cm_prob, cmap=cmap, linewidths=0.5, linecolor=\"white\")\n    plt.show()\n    plt.close()",
      "block_group": "82da29b3b88a4884ae27a5a050a1389e",
      "execution_count": 82,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgOzJSDWtU1i",
        "colab": {
          "height": 311,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe34615-099c-4e3f-db98-31debe6601d9",
        "cell_id": "00687552537d46638466cbe71246fc43",
        "deepnote_cell_type": "code"
      },
      "source": "history = torch.load(\"/content/histories/FootballTransformer_history_epoch1.pt\", weights_only=False)\nplot_conf_mat(history)",
      "block_group": "cc4ac9fb19814b8da1181942173f76d7",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Shape of passed values is (27, 27), indices imply (28, 28)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2210232273.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/histories/FootballTransformer_history_epoch1.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_conf_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3786629054.py\u001b[0m in \u001b[0;36mplot_conf_mat\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Turn confusion matrix into pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mval_cm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_conf_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevent_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevent_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Divide each colum entry by the row sum to get P(pred|target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    825\u001b[0m                 )\n\u001b[1;32m    826\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    828\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (27, 27), indices imply (28, 28)"
          ]
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xYyfcfqtU1i",
        "cell_id": "32056fe1a33d420c8deb7346cf1eb942",
        "deepnote_cell_type": "markdown"
      },
      "source": "## Discussion\nIn this project we have shown that a transformer model can predict the next event in a football match better than predicting the majority class. However, this conclusion comes with a few important caveats. The LSTM model outperforms the transformer and while not predicting pass every time, the transformer and LSTM model predict clearly predict it for most preceding events, as can be seen from the confusion matrix. Of course there is room for improvement. First, more data is needed to make the model more accurate. Given the billions of tokens, successful LLMs are trained on, the ~4 million data points this model was trained on seems insufficient. Second, it would be interesting to see the models' performance when a currect prediction of a pass is weighted less compared to the correct prediction of all over events. We would expect that with such a weighted loss function, the model would find deeper structure and predict events more precisely. Next, the data could be engineered better: event classes could be collapsed so the model doesn't need to predict all possible events. Similarly, instead of adding continuous and categorical features together, features could be concatenated in order to lose less information. Additionally, end coordinates could be predicted with a separate continuous loss function, which we would expect would lead to the model better 'understanding' whhat a pitch is etc.  \nFinally, following the logic of generative language models (e.g. GPT), we would hope that optimizing the transformer model in this project would lead to the possibility of a generative model. Such a model could be fed the beginning of a match and then predict the outcome of a match.\n* Also mention here that it would be important to include the position of the players on the pitch as a feature\n\n**Generalizability**\nGiven the good performance of this model, we would expect this model to perform equally well on other sports data (e.g. Basketball, American football). Further, with event embeddings similarly to the ones used here and in Simpson et al. (2022), any sequence of events that has some inherent structure should be predictable by such a model (e.g. human day-to-day activities), given sufficient training data.\n### Takeaways?\n* Transformers well-suited for predictions within structures sequences",
      "block_group": "96cc29add9414235bf20f847947f46e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIjFQ8e8tU1j",
        "cell_id": "1f8d722f092449abbe968d04f0d8f3dc",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 6. Division of labour\n**Everyone contributed equally**",
      "block_group": "49fdaf2093b8448f8a382b3958ec73af"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HCagNlHtU1j",
        "cell_id": "b767d6bd3a36428681f3f9218de40ab7",
        "deepnote_cell_type": "markdown"
      },
      "source": "",
      "block_group": "8c0e03b6213a42e69efb0b24d4a972be"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bABFCYLNtU1j",
        "cell_id": "a8f9d394ed7e4a6abc270125262d7883",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 7. References\nPappalardo, L., Cintia, P., Rossi, A., Massucco, E., Ferragina, P., Pedreschi, D., & Giannotti, F. (2019). A public data set of spatio-temporal match events in soccer competitions. *Scientific Data, 6*(1), 236. https://doi.org/10.1038/s41597-019-0247-7\n\nSimpson, I., Beal, R. J., Locke, D., & Norman, T. J. (2022). Seq2Event: Learning the language of soccer using transformer-based match event prediction. *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, 3898–3908. https://doi.org/10.1145/3534678.3539138\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. *Advances in Neural Information Processing Systems, 30*.\n\n\n",
      "block_group": "ee88882c9dc5429faa743608672261e4"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a7159be7-402c-457e-b158-4e16e892eaf8' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_persisted_session": {
      "createdAt": "2025-12-19T22:20:49.830Z"
    },
    "deepnote_notebook_id": "b5ba803b330e45c0a43b063b6c311ee3"
  }
}